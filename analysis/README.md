# Simple Analysis Example

This is a simple example that demonstrates the analysis service workflow:

1. **Receives**: 2 CSV files + 1 parameter (multiplier)
2. **Processes**: Loads CSV files with pandas, multiplies parameter by 2
3. **Returns**: 2 XLSX files + KPIs with the result

## Configuration Files

### `analysis/config/inputs.py` - Input File Configuration

This file defines what input files the analysis expects:

```python
INPUT_FILES = {
    "file1": {
        "dtype": "csv",
        "name": "file1.csv",
        "description": "First CSV input file",
        "path": "file1.csv",
        "required": True
    },
    "file2": {
        "dtype": "csv",
        "name": "file2.csv", 
        "description": "Second CSV input file",
        "path": "file2.csv",
        "required": True
    }
}
```

### `analysis/config/outputs.py` - Output File Configuration

This file defines what output files the analysis generates:

```python
OUTPUT_FILES = {
    "output1": {
        "dtype": "xlsx",
        "name": "output1.xlsx",
        "description": "First processed file saved as XLSX",
        "path": "output1.xlsx",
        "required": True
    },
    "output2": {
        "dtype": "xlsx",
        "name": "output2.xlsx", 
        "description": "Second processed file saved as XLSX",
        "path": "output2.xlsx",
        "required": True
    },
    "kpis": {
        "dtype": "yaml",
        "name": "kpis.yaml",
        "description": "KPIs in YAML format",
        "path": "kpis.yaml",
        "required": True
    }
}
```

### `analysis/config/parameters.py` - Parameter Configuration

This file defines what parameters the analysis expects:

```python
MULTIPLIER = {
    "basename": "multiplier",
    "type": "number",
    "description": "Number to multiply by 2",
    "required": True,
    "default": 1
}
```

### `analysis/run.py` - Analysis Logic

This is where you implement your **actual analysis logic**:

```python
def run_analysis(input_files, parameters, output_files, processing_files, progress_callback):
    # Load 2 CSV files
    df1 = pd.read_csv(input_files["file1"])
    df2 = pd.read_csv(input_files["file2"])
    
    # Get parameter and calculate result
    multiplier = parameters.get('multiplier', 1)
    result = 2 * multiplier
    
    # Save as XLSX files
    df1.to_excel(output_files["output1"], index=False)
    df2.to_excel(output_files["output2"], index=False)
    
    # Save KPIs
    kpis = {"multiplier": multiplier, "result": result}
    with open(output_files["kpis"], 'w') as f:
        yaml.dump(kpis, f)
    
    return {"multiplier": multiplier, "result": result}
```

## File Structure

```
analysis/
├── config/
│   ├── config.py     # ← Main config that imports from others
│   ├── inputs.py     # ← Edit this to configure input files
│   ├── outputs.py    # ← Edit this to configure output files
│   └── parameters.py # ← Edit this to configure parameters
├── data/
│   ├── inputs/       # ← Files received from orchestrator
│   ├── outputs/      # ← Files generated by analysis
│   └── processed/    # ← Temporary processing files
├── src/              # ← Empty, for helper functions
├── run.py            # ← Edit this to implement analysis logic
└── README.md
```

## Examples

### Adding a New Input File

1. Edit `analysis/config/inputs.py`:
```python
INPUT_FILES = {
    "data": {
        "dtype": "csv",
        "name": "data.csv",
        "description": "Main data file",
        "path": "data.csv",
        "required": True
    },
    "weather": {  # ← Add new file
        "dtype": "csv",
        "name": "weather_data.csv",
        "description": "Weather data for analysis",
        "path": "weather_data.csv",
        "required": False
    }
}
```

2. Use it in `analysis/run.py`:
```python
weather_file = input_files.get("weather")
if weather_file:
    weather_df = pd.read_csv(weather_file)
```

### Adding a New Output File

1. Edit `analysis/config/outputs.py`:
```python
OUTPUT_FILES = {
    "results": {
        "dtype": "json",
        "name": "results.json",
        "description": "Analysis results",
        "path": "results.json",
        "required": True
    },
    "charts": {  # ← Add new output
        "dtype": "png",
        "name": "charts.png",
        "description": "Analysis charts and visualizations",
        "path": "charts.png",
        "required": False
    }
}
```

2. Generate it in `analysis/run.py`:
```python
# Generate charts
plt.savefig(output_files["charts"])
```

### Custom KPIs

The system automatically saves KPIs in `kpis.yaml`. You can customize what gets saved:

```python
# In run_analysis function
kpis = {
    "total_deliveries": total_deliveries,
    "avg_delivery_time_minutes": round(avg_delivery_time, 2),
    "success_rate_percent": round(success_rate, 2),
    "custom_metric": your_custom_calculation,  # ← Add custom KPIs
    "analysis_metadata": {
        "parameters_used": parameters,
        "data_shape": list(data_df.shape),
        "analysis_timestamp": pd.Timestamp.now().isoformat()
    }
}
```

## Environment Variables

Set these environment variables for the service:

- `ORCHESTRATOR_URL`: URL of the orchestrator service (default: http://localhost:8001)
- `PORT`: Port for the analysis service (default: 8000)

## Running the Analysis

The analysis runs automatically when called by the orchestrator. For local testing:

```bash
python main.py <orchestrator_url> <job_id>
```

## Tips for Data Scientists

1. **Keep it simple**: Only modify files in `config/` and `run.py`
2. **Use YAML**: Parameters and KPIs are in YAML format for easy editing
3. **File paths**: All file paths are handled automatically - just use the keys from `INPUT_FILES` and `OUTPUT_FILES`
4. **Progress updates**: Use the `progress_callback` to show analysis progress
5. **Error handling**: The system handles file I/O errors automatically
6. **Helper functions**: Add utility functions in `src/` directory
